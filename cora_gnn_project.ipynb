{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6b487e3",
   "metadata": {},
   "source": [
    "# Cora Graph Neural Networks Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ada848",
   "metadata": {},
   "source": [
    "Author: Ankush Joshi   \n",
    "Date: 30 August, 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3d1f47",
   "metadata": {},
   "source": [
    "The goal of this project is to predict the subject category of a paper by using its own words and the structure of the citation work. The core idea is that a paper about Neural Networks is likely to cite another paper about Neural Networks and this justification gives the GNN a powerful advantage over models since they are all connected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56a999d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384d59bb",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea55142",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn.functional as F \n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.nn import GCNConv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e1ad64",
   "metadata": {},
   "source": [
    "## Loading in Cora Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f13d206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing to load Cora Dataset\n"
     ]
    }
   ],
   "source": [
    "print(\"Preparing to load Cora Dataset\")\n",
    "dataset = Planetoid(root='.', name='Cora')\n",
    "data = dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4b3638",
   "metadata": {},
   "source": [
    "## Gathering Information about data set  \n",
    "Number of nodes, edges, features, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0549d31d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cora Dataset\n",
      "----------------------------------------------\n",
      "Dataset: Cora\n",
      "==============================================\n",
      "Number of nodes (papers): 2708\n",
      "Number of edges (citations): 10556\n",
      "Number of features per node (words): 1433\n",
      "Number of classes (subjects): 7\n",
      "Number of Training nodes: 140\n",
      "Number of Validation nodes: 500\n",
      "Numbner of Test Nodes: 1000\n",
      "==============================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nCora Dataset\")\n",
    "print(\"----------------------------------------------\")\n",
    "print(f\"Dataset: {dataset.name}\")\n",
    "print(\"==============================================\")\n",
    "print(f\"Number of nodes (papers): {data.num_nodes}\")\n",
    "print(f\"Number of edges (citations): {data.num_edges}\")\n",
    "print(f\"Number of features per node (words): {dataset.num_node_features}\")\n",
    "print(f\"Number of classes (subjects): {dataset.num_classes}\")\n",
    "print(f\"Number of Training nodes: {data.train_mask.sum()}\")\n",
    "print(f\"Number of Validation nodes: {data.val_mask.sum()}\")\n",
    "print(f\"Numbner of Test Nodes: {data.test_mask.sum()}\")\n",
    "print(\"==============================================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12c38ca",
   "metadata": {},
   "source": [
    "## Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "17d3cddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class will define our Graph Convolutional Network (GCN)\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, num_classes):\n",
    "        super().__init__()\n",
    "        # GNN Layer that takes node features and learns a 16-dimensional embedding\n",
    "        self.conv1 = GCNConv(num_node_features, 16)\n",
    "        # A second GNN Layer that takes the 16 dimensional embedding and outputs predictions per class\n",
    "        self.conv2 = GCNConv(16, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        \n",
    "        # Pass data through first layer and apply a non-linear activation formula\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        # Apply dropout for regularization\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        # Pass through final layer for prediction\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        # Return the log-softmax probabilites for each class\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136036a4",
   "metadata": {},
   "source": [
    "## Training and Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4eeb0254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if GPU is avaiable, otherwise use CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d67b8456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Model\n",
      "Epoch: [20/200, Loss: 0.2356]\n",
      "Epoch: [40/200, Loss: 0.0669]\n",
      "Epoch: [60/200, Loss: 0.0438]\n",
      "Epoch: [80/200, Loss: 0.0394]\n",
      "Epoch: [100/200, Loss: 0.0291]\n",
      "Epoch: [120/200, Loss: 0.0229]\n",
      "Epoch: [140/200, Loss: 0.0329]\n",
      "Epoch: [160/200, Loss: 0.0446]\n",
      "Epoch: [180/200, Loss: 0.0181]\n",
      "Epoch: [200/200, Loss: 0.0211]\n",
      "Training Complete\n"
     ]
    }
   ],
   "source": [
    "# Moving data and model to selected device based on previous line\n",
    "model = GCN(dataset.num_node_features, dataset.num_classes).to(device)\n",
    "data = data.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "model.train()\n",
    "\n",
    "print(\"\\nTraining Model\")\n",
    "for epoch in range(200):        \n",
    "    optimizer.zero_grad()       # Clearing Gradients from previous step\n",
    "    out = model(data)           # Perform a single forward pass through the model\n",
    "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask]) # Calculate loss only on training nodes\n",
    "    loss.backward()             # Derive gradients\n",
    "    optimizer.step()            # Update model parameters\n",
    "\n",
    "    if(epoch + 1) % 20 == 0:\n",
    "        print(f'Epoch: [{epoch+1}/200, Loss: {loss.item():.4f}]')\n",
    "\n",
    "print(\"Training Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e395ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Making predictions on the entire graph\n",
    "pred = model(data).argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a587590a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Accuracy on Test set: 0.8010\n"
     ]
    }
   ],
   "source": [
    "# Checking accuracy on test nodes\n",
    "correct = (pred[data.test_mask] == data.y[data.test_mask]).sum()\n",
    "acc = int(correct) / int(data.test_mask.sum())\n",
    "print(f\"\\nFinal Accuracy on Test set: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9de10c5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e37e97",
   "metadata": {},
   "source": [
    "## Testing Model Predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9385aa8b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[52]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Compiling a list of class names based on Cora dataset\u001b[39;00m\n\u001b[32m      2\u001b[39m class_names = [\n\u001b[32m      3\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mTheory\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mReinforcement_Learning\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mGenetic_Algorithms\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      4\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mNeural_Networks\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mProbabilistic_Methods\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mCase_Based\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mRule_Learning\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      5\u001b[39m ]\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m test_node_indices = \u001b[43mnp\u001b[49m.where(data.test_mask.cpu().numpy())[\u001b[32m0\u001b[39m]\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m10\u001b[39m):\n\u001b[32m     10\u001b[39m     node_index = test_node_indices[i]\n",
      "\u001b[31mNameError\u001b[39m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# Compiling a list of class names based on Cora dataset\n",
    "class_names = [\n",
    "    \"Theory\", \"Reinforcement_Learning\", \"Genetic_Algorithms\",\n",
    "    \"Neural_Networks\", \"Probabilistic_Methods\", \"Case_Based\", \"Rule_Learning\"\n",
    "]\n",
    "\n",
    "test_node_indices = np.where(data.test_mask.cpu().numpy())[0]\n",
    "\n",
    "for i in range(10):\n",
    "    node_index = test_node_indices[i]\n",
    "    predicted_class_index = pred[node_index].item()\n",
    "    true_class_index = data.y[node_index].item()\n",
    "\n",
    "    predicted_class_name = class_names[predicted_class_index]\n",
    "    true_class_name = class_names[true_class_index]\n",
    "    is_correct = \"Correct\" if predicted_class_index == true_class_index else \"Incorrect\"\n",
    "\n",
    "    print(f\"Node {node_index}:\")\n",
    "    print(f\"\\tPredicted Class: {predicted_class_name}\")\n",
    "    print(f\"\\tTrue Class:      {true_class_name}\")\n",
    "    print(f\"\\tResult:          {is_correct}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
